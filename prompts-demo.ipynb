{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61e515b6",
   "metadata": {},
   "source": [
    "## Getting Started with prompt Template and Chat Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e27cd0",
   "metadata": {},
   "source": [
    "### Use PromptTemplate to create a template for a string prompt.\n",
    "By default, PromptTemplate uses Python’s str.format syntax for templating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c45795a",
   "metadata": {},
   "source": [
    "### Install the following libraries if not installed already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2213d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install langchain-community\n",
    "# !pip install langchain-openai\n",
    "# !pip install python-dotenv\n",
    "# !pip install langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573017aa",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71fc036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b963a811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51026ef",
   "metadata": {},
   "source": [
    "### Use your model of Choice - OpenAI or Google's Gemini. \n",
    "We are using the ChatCompletion APIs of these LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "467b6c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"), temperature=0.5)\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\",  google_api_key=os.getenv(\"GOOGLE_API_KEY\"), temperature=0.5, convert_system_message_to_human=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf814975",
   "metadata": {},
   "source": [
    "### A simple string based Prompt formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fc64f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diksh\\.virtualenvs\\rag-app-Nid9tYsN\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='An old man is sitting on a park bench in Manchester, staring up at the thick, grey clouds.\\n\\nA tourist sits down next to him and says cheerfully, \"A bit miserable today, isn\\'t it?\"\\n\\nThe old man sighs, never taking his eyes off the sky. \"Son,\" he says, \"I\\'ve lived here for 80 years. We don\\'t have weather. We just have different types of disappointment.\"' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-pro', 'safety_ratings': []} id='run--d4a4a02b-645f-4818-8b54-0efe3d7fa5f6-0' usage_metadata={'input_tokens': 10, 'output_tokens': 91, 'total_tokens': 1520, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1419}}\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "prompt = prompt_template.format(adjective=\"sad\", content=\"British Weather\")\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ce0254",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate: The prompt to chat models is a list of chat messages.\n",
    "Each chat message is associated with content, and an additional parameter called role. <br>\n",
    "For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "611c91b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57a2d713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diksh\\.virtualenvs\\rag-app-Nid9tYsN\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"My name is Bob!\\n\\nAs a helpful AI, I'm good at a lot of things. You can think of me as a versatile assistant. For example, I can:\\n\\n*   **Answer your questions** on a huge range of topics.\\n*   **Brainstorm ideas** with you for a project or story.\\n*   **Write and edit text**, like emails, essays, or even poems.\\n*   **Summarize** long articles or documents for you.\\n*   **Explain complex subjects** in a simple way.\\n*   **Help you with coding** and debugging.\\n\\nBasically, I'm here to help you learn, create, and get things done. What's on your mind?\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-pro', 'safety_ratings': []} id='run--f1e374ab-1daf-47be-a1f8-2e0645a98849-0' usage_metadata={'input_tokens': 41, 'output_tokens': 151, 'total_tokens': 1081, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 889}}\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "prompt = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name and what are you good at?\")\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075ef76f",
   "metadata": {},
   "source": [
    "### Various ways of formatting System/Human/AI prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af82cb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='I prefer other genres over action movies!'\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"\"\"You are a helpful assistant that re-writes the user's text to \n",
    "                sound more upbeat.\"\"\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "prompt = chat_template.format_messages(text=\"I don't like action movies\")\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c0e5e",
   "metadata": {},
   "source": [
    "### Providing a Context along with the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b7973c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: Which libraries and model providers offer LLMs?\n",
    "\n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "73cb2a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.\"\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f00e1d",
   "metadata": {},
   "source": [
    "### Langchain’s ```FewShotPromptTemplate``` caters to source knowledge input. The idea is to “train” the model on a few examples — we call this few-shot learning — and these examples are given to the model within the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b8f66",
   "metadata": {},
   "source": [
    "The goal of few-shot prompt templates are to dynamically select examples based on an input, and then format the examples in a final prompt to provide for the model.<br>\n",
    "\n",
    "<b>Fixed Examples</b><br>\n",
    "The most basic (and common) few-shot prompting technique is to use a fixed prompt example. This way you can select a chain, evaluate it, and avoid worrying about additional moving parts in production. <br>\n",
    "\n",
    "The basic components of the template are: - examples: A list of dictionary examples to include in the final prompt. - example_prompt: converts each example into 1 or more messages through its format_messages method. A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "15e22ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, \n",
    "    {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "09912e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt example from above template\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative  and funny responses to the users questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d23b00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e8c7b333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "\n",
      "User: What movie should I watch this evening?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"What movie should I watch this evening?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "29ca44a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='How about \"The Shawshank Redemption\"? It\\'s a classic that will make you question your own existence.')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = few_shot_prompt_template | model\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c6a0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-app-Nid9tYsN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
